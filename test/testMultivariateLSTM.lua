-- Test use of multivariate LSTMs with Philippe's models

-- Imports
require 'unsup'
require 'optim'
require 'torch'
require 'nninit'
local ts_init = require 'TSInitialize'
require 'importTSDataset'
require 'mainLearning'
require 'mainParameters'
require 'mainProfiler'
require 'mainUtils'
require 'modelCriterias'

require 'moduleSlidingWindow'
require 'modelLSTM'


cmd = torch.CmdLine()
cmd:option('--useCuda', false, 'whether to enable CUDA processing')

-- parse input params
cmd_params = cmd:parse(arg)

local useCuda = cmd_params.useCuda

ts_init.set_cuda({cuda=useCuda})

-- Create a default configuration
options = setDefaultConfiguration();
-- Override some parameters
options.visualize = true;

options.featSize = 12;
options.sequenceLength = 128;
options.batchSize = 64;
options.sliceSize = 32;

options.windowSize = 16
options.windowStep = 1

options.cuda = useCuda

curModel = modelLSTM(options);

-- Define structure
structure = {};
structure.nLayers = 3;
structure.nInputs = options.sliceSize
structure.nFeats = 12;
structure.layers = {200, 100, 50, 20};
structure.nOutputs = options.sliceSize;
structure.convSize = {16, 32, 64};
structure.kernelWidth = {8, 8, 8};
structure.poolSize = {2, 2, 2};
structure.nClassLayers = 3;

model = curModel:defineModel(structure, options)

if useCuda then model:cuda() end

print(model)

-- For simplicity, the multi-variate dataset in this example is independently distributed.
-- Toy dataset (task is to predict next vector, given previous vectors) following the normal distribution .
-- Generated by sampling a separate normal distribution for each random variable.
-- note: vX is used as both input X and output Y to save memory
local function evalPDF(vMean, vSigma, vX)
   for i=1,vMean:size(1) do
      local b = (vX[i]-vMean[i])/vSigma[i]
      vX[i] = math.exp(-b*b/2)/(vSigma[i]*math.sqrt(2*math.pi))
   end
   return vX
end

assert(options.featSize > 1, "Multi-variate time-series")

vBias = torch.randn(options.featSize)
vMean = torch.Tensor(options.featSize):fill(5)
vSigma = torch.linspace(1,options.featSize,options.featSize)
sequence = torch.Tensor(options.sequenceLength, options.featSize)

j = 0
for i=1,options.sequenceLength do
   sequence[{i,{}}]:fill(j)
   evalPDF(vMean, vSigma, sequence[{i,{}}])
   sequence[{i,{}}]:add(vBias)
   j = j + 1
   if j>10 then j = 0 end
end
-- print('Sequence:'); print(sequence)

if useCuda then sequence = sequence:cuda() end

-- batch mode

offsets = torch.LongTensor(options.batchSize):random(1,options.sequenceLength)

-- train rnn model
minErr = math.huge -- options.featSize -- report min error
minK = 0
options.nIterations = 100
options.learningRate = 1e-3
avgErrs = torch.Tensor(options.nIterations):fill(0)

local inputs
if useCuda then
   inputs = torch.CudaTensor(options.sliceSize, options.batchSize, options.featSize)
else
   inputs = torch.Tensor(options.sliceSize, options.batchSize, options.featSize)
end
local targets = inputs:new()

if useCuda then inputs:cuda(); targets:cuda() end
   
criterion = nn.MSECriterion()
criterion = nn.SequencerCriterion(criterion)

if useCuda then criterion:cuda() end

for k = 1, options.nIterations do
   -- 1. create a sequence of rho time-steps
   for step = 1, options.sliceSize do
      -- batch of inputs
      inputs[step] = inputs[step] or sequence.new()
      inputs[step]:index(sequence, 1, offsets)
      -- batch of targets
      offsets:add(1) -- increase indices by 1
      offsets[offsets:gt(options.sequenceLength)] = 1
      targets[step] = targets[step] or sequence.new()
      targets[step]:index(sequence, 1, offsets)
   end
   -- 2. forward sequence through rnn

   local outputs = model:forward(inputs)
   if useCuda then outputs:cuda() end

   -- if useCuda then targets:cuda() end
   local err = criterion:forward(outputs, targets)
   
   -- report errors
   
   print('Iter: ' .. k .. '   Err: ' .. err)
   avgErrs[k] = err
   if avgErrs[k] < minErr then
      minErr = avgErrs[k]
      minK = k
   end

   -- 3. backward sequence through rnn (i.e. backprop through time)
   
   model:zeroGradParameters()
   
   local gradOutputs = criterion:backward(outputs, targets)
   local gradInputs = model:backward(inputs, gradOutputs)

   -- 4. updates parameters
   
   model:updateParameters(options.learningRate)
   --end
end

print('min err: ' .. minErr .. ' on iteration ' .. minK)
