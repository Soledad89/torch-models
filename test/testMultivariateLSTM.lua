-- Try and use multivariate LSTMs with Philippe's models

-- Imports
require 'unsup'
require 'optim'
require 'torch'
require 'nninit'
require 'importTSDataset'
require 'mainLearning'
require 'mainParameters'
require 'mainProfiler'
require 'mainUtils'
require 'modelCriterias'

require 'moduleSlidingWindow'
require 'modelLSTM'

-- Create a default configuration
options = setDefaultConfiguration();
-- Override some parameters
options.visualize = true;

options.featSize = 12;
options.sequenceLength = 128;
options.batchSize = 64;
options.sliceSize = 32;
curModel = modelLSTM();

curModel:parametersDefault();

-- Define structure
structure = {};
structure.nLayers = 3;
structure.nInputs = options.featSize;
structure.layers = {100, 100, 50, 20};
structure.nOutputs = 12;
structure.convSize = {16, 32, 64};
structure.kernelWidth = {8, 8, 8};
structure.poolSize = {2, 2, 2};
structure.nClassLayers = 3;

model = curModel:defineModel(structure, options)

-- For simplicity, the multi-variate dataset in this example is independently distributed.
-- Toy dataset (task is to predict next vector, given previous vectors) following the normal distribution .
-- Generated by sampling a separate normal distribution for each random variable.
-- note: vX is used as both input X and output Y to save memory
local function evalPDF(vMean, vSigma, vX)
   for i=1,vMean:size(1) do
      local b = (vX[i]-vMean[i])/vSigma[i]
      vX[i] = math.exp(-b*b/2)/(vSigma[i]*math.sqrt(2*math.pi))
   end
   return vX
end

assert(options.featSize > 1, "Multi-variate time-series")

vBias = torch.randn(options.featSize)
vMean = torch.Tensor(options.featSize):fill(5)
vSigma = torch.linspace(1,options.featSize,options.featSize)
sequence = torch.Tensor(options.sequenceLength, options.featSize)

j = 0
for i=1,options.sequenceLength do
  sequence[{i,{}}]:fill(j)
  evalPDF(vMean, vSigma, sequence[{i,{}}])
  sequence[{i,{}}]:add(vBias)
  j = j + 1
  if j>10 then j = 0 end
end
-- print('Sequence:'); print(sequence)

-- batch mode

offsets = torch.LongTensor(options.batchSize):random(1,options.sequenceLength)

-- 1. create a sequence of rho time-steps

local inputs = torch.Tensor(options.sliceSize, options.batchSize, options.featSize)
local targets = inputs:new()
for step = 1, options.sliceSize do
   -- batch of inputs
   inputs[step] = inputs[step] or sequence.new()
   inputs[step]:index(sequence, 1, offsets)
   -- batch of targets
   offsets:add(1) -- increase indices by 1
   offsets[offsets:gt(options.sequenceLength)] = 1
   targets[step] = targets[step] or sequence.new()
   targets[step]:index(sequence, 1, offsets)
end

model:forward(inputs)

criterion = nn.MSECriterion()
criterion = nn.SequencerCriterion(criterion)

criterion:forward(inputs, targets)
