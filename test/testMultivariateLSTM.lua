-- Try and use multivariate LSTMs with Philippe's models

-- Imports
require 'unsup'
require 'optim'
require 'torch'
require 'nninit'
require 'importTSDataset'
require 'mainLearning'
require 'mainParameters'
require 'mainProfiler'
require 'mainUtils'
require 'modelCriterias'

require 'moduleSlidingWindow'
require 'modelLSTM'

-- Create a default configuration
options = setDefaultConfiguration();
-- Override some parameters
options.visualize = true;

options.featSize = 12;
options.sequenceLength = 128;
options.batchSize = 64;
options.sliceSize = 32;

options.windowSize = 16
options.windowStep = 1
curModel = modelLSTM(options);

curModel:parametersDefault();

-- Define structure
structure = {};
structure.nLayers = 3;
structure.nInputs = options.sliceSize
structure.nFeats = 12;
structure.layers = {200, 100, 50, 20};
structure.nOutputs = options.sliceSize;
structure.convSize = {16, 32, 64};
structure.kernelWidth = {8, 8, 8};
structure.poolSize = {2, 2, 2};
structure.nClassLayers = 3;

model = curModel:defineModel(structure, options)

print(model)

-- For simplicity, the multi-variate dataset in this example is independently distributed.
-- Toy dataset (task is to predict next vector, given previous vectors) following the normal distribution .
-- Generated by sampling a separate normal distribution for each random variable.
-- note: vX is used as both input X and output Y to save memory
local function evalPDF(vMean, vSigma, vX)
   for i=1,vMean:size(1) do
      local b = (vX[i]-vMean[i])/vSigma[i]
      vX[i] = math.exp(-b*b/2)/(vSigma[i]*math.sqrt(2*math.pi))
   end
   return vX
end

assert(options.featSize > 1, "Multi-variate time-series")

vBias = torch.randn(options.featSize)
vMean = torch.Tensor(options.featSize):fill(5)
vSigma = torch.linspace(1,options.featSize,options.featSize)
sequence = torch.Tensor(options.sequenceLength, options.featSize)

j = 0
for i=1,options.sequenceLength do
  sequence[{i,{}}]:fill(j)
  evalPDF(vMean, vSigma, sequence[{i,{}}])
  sequence[{i,{}}]:add(vBias)
  j = j + 1
  if j>10 then j = 0 end
end
-- print('Sequence:'); print(sequence)

-- batch mode

offsets = torch.LongTensor(options.batchSize):random(1,options.sequenceLength)

-- train rnn model
minErr = math.huge -- options.featSize -- report min error
minK = 0
options.nIterations = 100
options.learningRate = 1e-3
avgErrs = torch.Tensor(options.nIterations):fill(0)
for k = 1, options.nIterations do
   -- 1. create a sequence of rho time-steps
   local inputs = torch.Tensor(options.sliceSize, options.batchSize, options.featSize)
   local targets = inputs:new()
   for step = 1, options.sliceSize do
      -- batch of inputs
      inputs[step] = inputs[step] or sequence.new()
      inputs[step]:index(sequence, 1, offsets)
      -- batch of targets
      offsets:add(1) -- increase indices by 1
      offsets[offsets:gt(options.sequenceLength)] = 1
      targets[step] = targets[step] or sequence.new()
      targets[step]:index(sequence, 1, offsets)
   end
   
   --for batchInd=1,options.batchSize do
      --local inputs = inputs:select(2, batchInd)
      --local targets = targets:select(2, batchInd)
      
      criterion = nn.MSECriterion()
      criterion = nn.SequencerCriterion(criterion)

      -- 2. forward sequence through rnn

      local outputs = model:forward(inputs)
      local err = criterion:forward(outputs, targets)
      
      -- report errors
      
      print('Iter: ' .. k .. '   Err: ' .. err)
      avgErrs[k] = err
      if avgErrs[k] < minErr then
	 minErr = avgErrs[k]
	 minK = k
      end

      -- 3. backward sequence through rnn (i.e. backprop through time)
      
      model:zeroGradParameters()
      
      local gradOutputs = criterion:backward(outputs, targets)
      local gradInputs = model:backward(inputs, gradOutputs)

      -- 4. updates parameters
      
      model:updateParameters(options.learningRate)
   --end
end

print('min err: ' .. minErr .. ' on iteration ' .. minK)
